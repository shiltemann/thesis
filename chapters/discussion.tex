\begin{savequote}[75mm]
``Humans are allergic to change. They love to say, `We've always done it this way.'' I try to fight that. That's why I have a clock on my wall that runs counter-clockwise.''
\qauthor{Grace Hopper}
\end{savequote}

\chapter{Discussion}\label{discussion}
\setcounter{figure}{-1}
\setcounter{table}{-1}
\setcounter{section}{-1}
\setcounter{NAT@ctr}{-1}

Since the completion of the human reference genome in 2003, the field of molecular biology has been transformed almost beyond recognition, and along with it, the field of bioinformatics has evolved from a niche discipline to an integral part of every biomolecular research question. Where mere decades ago most genomic data analyses could largely be performed by hand, nowadays they often require a supercomputer. Programming knowledge is required to run such analyses, but biologists are not typically trained in these skills, and have thus become reliant on bioinformaticians to carry out this work for them. Similarly, bioinformaticians often lack the increasingly complex biological knowledge required to fully interpret the results~\cite{preeyanon2014reproducible}. Biologist and bioinformatician must therefore work together closely, and each should be trained in the other discipline in order gain awareness of factors that might influence analysis or interpretation. Furthermore, data is being generated at an exponential rate (while bioinformaticians are not), and one way to address this gap is to empower researchers to run their own day-to-day analyses without the need to consult a bioinformatician. This can be achieved through creation of accessible analysis platforms and training resources.


\section{Galaxy for open and accessible research}
The Galaxy project~\cite{giardine2005galaxy,blankenberg2010galaxy,afgan2016galaxy} (with the latest update paper in \textbf{Chapter} \ref{chapter:general}) is a framework that enables such empowerment of researchers to run complex data analysis without programming expertise. The Galaxy project is free and open-source and community-driven, encouraging feedback and code contributions from its user community to improve and evolve along with the ever-changing landscape that is bioinformatics. \hyperref[chapter:galaxy]{\textbf{Chapter~\ref{chapter:general}}} outlines the ongoing development in the Galaxy framework. The set of tools wrapped into Galaxy by both the user community and the core development team continues to increase exponentially. New core features are continually incorporated into the Galaxy code base in order to facilitate the needs of users in the ever-changing landscape of bioinformatics.

Scalability is an important area of development, and continual efforts are made to ensure the platform can keep up with the current \emph{big data} explosion. To this end, several improvement were made to assist users with the handling of large datasets, such as dataset collections and the rule-based uploader. In terms of server scalability, Galaxy supports integration with a multitude of workload managers and compute infrastructures.

Accessibility of the Galaxy platform was improved by the integration of a new help forum, the introduction of interactive tours to faimilarize users with the Galaxy interface, and through the development of a central repository of Galaxy-based training materials (\textbf{Chapter~\ref{chapter:training}}).


\section{Training}
Training is an essential component in the dissemination of accessible bioinformatics tools and workflows. The Galaxy platform is especially well-suited for the delivery of bioinformatics training because it provides a layer of abstraction that allows trainees to focus on the bioinformatics \emph{concepts} rather than the implementation details of the tools. Without this separation, trainees would have to simultaneously learn about the UNIX commandline or programming environment, as well as the bioinformatics topics at hand. This would increase the cognitive load and hamper the learning process~\cite{paas2003cognitive}. Given this observation of Galaxy's suitability for use in training, the Galaxy Training Network (GTN) was formed, a loosely-defined open group of instructors around the world who use Galaxy for training purposes. Initially there was little coordination between the different instructors in terms of materials used, and thus a lot of duplication of effort. There was a clear need for centralisation of training materials and knowledge sharing within the community. Chapter~\ref{chapter:training} describes the community-driven web-based framework for the delivery of bioinformatics training using the Galaxy platform that we developed in response to this need.

Our aim was to create a fully open and transparent framework that is accessible and easy to use for both learners and trainers. The materials are centered around \emph{research stories}; usually the recreation of results described in published papers. This gives learners the confidence that the tools and pipelines are practically useful and of publication-level quality, as well providing them with the opportunity to dive deeper into the science and informatics behind the training. Since the creation of this training platform, a number of scientific publications have included Galaxy training materials as a form of documentation and illustration of the presented analysis pipelines~\cite{gruning2017rna,blank2018disseminating,batut2017asaim,hiltemann2018galaxy}.

One of the main challenges in designing this framework was to allow easy contributions from instructors, without the need for any web development knowledge. To this end, we used Jekyll templating~\cite{url-jekyll}, which allows tutorials to be written in the simple and accessible markup language Markdown~\cite{url-markdown}, which is then automatically converted to a web page. Analogous to how Galaxy allows scientists to run analyses while being abstracted away from the implementation layer of the tools, so does this approach allow instructors to create web pages for their tutorials without being concerned with the web application layer.

A further challenge was to enable the materials to be usable both by instructors during workshops, and by individuals learning on their own. This is accomplished by including all materials instructors might provide during a workshop in the GTN training materials framework. This includes introduction slides as well as hand-on materials, data and workflows, further reading suggestions, and an automatically updated list of available Galaxy servers which meet the requirements to run a given tutorial. Furthermore, learning assessments are provided in the form of question boxes, answers to which are included within the materials (in an initially hidden state) for self-study learners. If further assistance is required, links to support channels (help forum, chat rooms) are provided.

The community-driven nature of the training framework is essential for the long-term survival of the project; it allows for the distribution of the maintenance burden and takes advantage of the combined expertise present in the community. All development happens on GitHub, where anybody may suggest additions or changes, and any such proposed changes are thoroughly tested using Travis continuous integration system~\cite{travis-ci} to ensure functionality and adherence to guidelines. The proposed changes or additions are then reviewed for by one or more of the topic maintainers and other volunteers from the community. Once approved, the code is merged into the main code base, and the new website is automatically built and deployed using Travis and GitHub. In order to assess the quality of the tutorials and identify areas of improvement, feedback from both learners and instructors is indispensable. To this end, we integrated evaluation forms at the end of each tutorial, and hold regular community meetings with training developers and trainers.

As Galaxy evolves, so will the associated tutorials; where Galaxy is expanding beyond bioinformatics and is now also being used in fields such as natural language processing and computational chemistry, so have we noticed a steady expansion of topics and tutorials contributed by the community. In the year following the publication of Chapter \ref{chapter:training} , we saw 6 new topics added, 66 new tutorials, and the number of contributors grew from 64 to 137.

Where the focus of development initially lay with improving the experience for end-users of the tutorials, our focus is now shifting to increasing support for tutorial contributors and instructors intending to use our materials. The main challenge in the coming years will be the community management; creating and sustaining a close-knit community of Galaxy users and instructors so that the project can survive even when its original developers have moved on.


\section{Visualisation and Reporting}
Galaxy is a highly flexible analysis platform, but this flexibility comes hand-in-hand with complexity. This trade-off is acceptable for researchers still developing their pipelines, but for clinicians who have fixed and validated workflows, the flexibility is not longer required, and in many cases even undesirable.

Galaxy in its current form lacks an appealing system of results summation and reporting. While Galaxy sports a plugin system for visualisation of individual datasets, a generic reporting tool for displaying a set of output datasets together does not exist. To this end, we developed iReport (Chapter~\ref{chapter:general}), a fully customizable Galaxy tool for the generation HTML reports capable of displaying any number of workflow outputs. iReport is intended as the final step of a workflow; in this manner, end-users need only to open a single workflow output to get a full overview of results. The developer of the workflow can configure an iReport fully tailored to the needs of their users, including the ability to add links to datasets and external resources, create searchable and sortable tables, embed images and custom text, and to divide content into different pages. By adding such a summarizing report, clinicians and other end users are able to run workflows and view results with minimal instruction or knowledge the Galaxy interface. Going even further, by using the Galaxy API, simplified web front-ends for Galaxy may be created (e.g. Galaksio~\cite{Klingstrm2017} and IRIDA~\cite{matthews2018integrated}), such that only features needed by the users are exposed. In the case of clinical applications, this provides the additional benefit of minimizing the chances of incorrect invocation of the validated workflows

\section{Use Cases}

The concepts and tools described in the previous sections were applied to two separate use cases. Chapters~\ref{chapter:fusiongenes} and~\ref{chapter:variants} describe the creation of analysis tools and pipelines for variant analysis in prostate cancer research. In Chapter~\ref{chapter:microbiota}, Galaxy-based analysis pipelines were developed and tested for the application of NGS-based microbiota profiling for clinical diagnostics.


\subsection{Cancer Analysis: Fusion Gene Detection}
\subsubsection{The Bio}

Prostate cancer is one of the most commonly diagnosed cancer types in men \cite{jemal2010global}, and while many patients present with an indolent form of the disease, others develop a highly aggressive tumour type, and overall, prostate cancer is among the top 10 leading causes of cancer-related deaths in men. The presence of the \emph{TMPRSS2-ERG} gene fusion is observed in approximately 50\% of prostate cancer patients~\cite{tomlins2005recurrent}, and many other less frequent fusions have also been identified. Fusion genes may arise when structural variants (SVs) occur within two different genes, causing the creation of hybrid genes with the potential to produce hybrid proteins that may disrupt vital cell functions. Detection of fusion genes may aid in diagnostics

Chromothripsis is a phenomenon observed primarily in cancer cells, that involves the shattering and subsequent imprecise repair of one or more chromosomes in a single catastrophic event. Chromothripsis is characterized by the observation of 1) large numbers of chromosomal rearrangements on a localized parts of the genome, 2) alternations between a small number of different copy number states, and 3) alternation between regions displaying a loss of heterozygosity (LOH) and those with preserved heterozygosity~\cite{maher2012chromothripsis}.

Chapter \ref{chapter:fusiongenes} describes the identification of chromothripsis in the 5q arm of the VCaP cell line. A very large number of SVs were described, and copy number varied primarily between two copy number states states, with only sporadic occurrences of a third state. This alternation between a small number of copy number states suggest the rearrangements were precipitated by a single catastrophic event.

The 573 rearrangements involving this part of chromosome 5 were then investigated for their potential to lead to the formation of fusion genes. Out of this large number of rearrangements, only 18 were found to occur between two different genes at consistent orientation. These fusion gene candidates were confirmed via sequencing using custom PCR. This allowed us to confirm 16 out of 18 fusion candidates on the DNA level, but only 5 of these were also measured on the mRNA level, suggesting instability of the fusion transcripts or down-regulation of gene expression.

Overall, this research has shown that any studies involving this commonly used prostate cancer cell line should take the presence of chromothripsis on chromosome 5q into account, as well as its utility as a model for research on chromothripsis.

\subsubsection{The Informatics}
Whole-genome sequence experiments typically generate very large output files. Furthermore, in the case of structural variant analysis, the output file formats lack consistency across tools and sequencing platforms, and interpretation is greatly helped by visualisation and annotation with external data resources. To this end, the iFUSE application was developed, creating a visual representation of fusion gene candidates and computing various metrics such as predictions of fusion protein product.

While iFUSE provides valuable aid in interpretation on a per-event basis through its visualisation and annotation, it is less suitable for getting a genome-wide overview of structural rearrangements. Large-scale rearrangements such as chromothripsis for instance are easy to miss when examining the raw textual output files or the iFUSE visualisations. In order to evaluate the presence of chromothripsis, a more high-level view of the genome is needed, and to that end, whole-genome visualisations were created using Circos~\cite{circos} for rearrangements, and GNUplot~\cite{url-gnuplot} for the copy number and heterozygosity plots. This approach enabled the instant identification of chromothripsis present in the VCaP sample, which could then be followed up by closer examination of individual rearrangements involving the chromosome 5q using iFUSE.

Both the iFUSE application, the Circos Galaxy wrappers, and the other tools and visualisations created in this chapter are open-source and publicly available on an example server, and are accompanied by extensive documentation to enable re-use by institutes that may have legal restrictions about the use of clinical samples on public servers. By using Galaxy


\subsection{Cancer Analysis: Somatic Mutation Determination}
\subsubsection{The Bio}
One of the main challenges in analysis of oncological samples is the identification of those mutations that potentially function as oncogenic drivers, and the large number of passenger mutations or polymorphisms present in the germline of the patient that are typically deemed to be functionally benign~\cite{lawrence2013mutational}. To this end, a sample of normal tissue from the same individual is often sequenced in conjunction with the tumour sample. This allows the subtraction of germline variants from the set of mutations detected in the cancer sample, in order to narrow down the set of potential driver mutations. However, in practice such an associated normal sample may not always be available, for a variety of reasons. In such cases, an alternative approach is required.

Given the observation that the majority of any individuals germline variants are polymorphic and observed frequently in the human population~\cite{TODO}, in combination with the exponential increase in publicly available genomic datasets, we explored the suitability of constructing a so-called \emph{virtual normal}, consisting of a reference set of variants found in the population. Chapter \ref{chapter:variants} describes this investigation.

Our approach used a set of over 900 publicly available whole genomes from healthy, ethnically diverse individuals. We combined this with the customary approach of annotating variants for presences in several online databases of polymorphism in the human population. We tested the performance of our method using 4 different tumour samples with matched normals, 2 of which had been sequenced on two different platforms (Complete Genomics and Illumina).

Our results show that while highly unique personal variants cannot be corrected for, a large number of polymorphisms may be removed without the need of an associated normal sample from the same individual, and as more and more WGS samples are made publicly available, ever rarer variants may be corrected for in this manner. Furthermore, we demonstrated that the use of a virtual normal consisting of whole-genome variant files provided a significant improvement over using only the online variant databases. This observation can be explained by noting that variants may be described in various different yet equivalent ways. These variant descriptions are not standardized, and it may not even be possible to do so. However, having knowledge of the area surrounding variants enables more advanced comparison algorithms to resolve equivalency of variants where routine position-based exact comparison method fall short. Therefore, comparing variants between samples, especially those sequenced on different platforms, is problematic without knowledge of surrounding variants in the same samples. This information is typically lost in online variant databases, which explains the increased performance of the virtual normal approach. Moreover, when using only an associated normal and online variant databases resulted in a roughly equal number of false-positive somatic variants (e.g. variants determined to be somatic but which were polymorphism based on virtual normal analysis), as did the approach without a matched normal. This can be explained by a combination of sequencing errors or incorrect variant calling in the associated normal samples, and the general difficulty presented in variant comparison analyses.

Where possible, using a combination of all three approaches -matched normal, virtual normal, and online polymorphism databases- yields optimal results, but in cases where no matched normal is available, relying on only the latter two correction sources provides an accurate alternative. Depending on the use case, the decrease in power to detect highly personal germline variants may be offset by the decrease in cost from the absence of necessity of sequencing a mathed normal sample with every tumour sample.

\subsubsection{The Informatics}
The samples in this study were sequenced by Complete Genomics~\cite{drmanac}, a sequencing service which delivers both raw sequencing data and post-processed results. They provide a suite of command line tools for handling and further analysis of their often custom file formats. As a first step towards building the virtual normal analysis pipelines, these existing tools were wrapped into Galaxy. On top of these third party tools, several custom analysis components had to be created, as well as several file format conversion steps to function as a \emph{glue} between steps. The full suite of analysis tools has been made publicly available on GitHub, as well as detailed instructions on where to obtain the virtual normal genome set, and how this may be extended with additional normal samples in the future.


\subsection{Microbiota Profiling for Clinical Diagnostics}
\subsubsection{The Bio}
The MYcrobiota project described in Chapter~\ref{chapter:microbiota} was aimed at developing a 16S microbiota profiling pipeline suitable for use in a clinical setting, in order to augment or eventually potentially replace the culture-based methods currently employed in microbial diagnostic laboratories. While 16S sequencing is a relatively well-established technique, there are several obstacles yet to overcome to enable its use in routine diagnostics. These obstacles include 1) the high prevalence of chimera formation during PCR amplification~\cite{huttenhower2012structure}, 2) the inability to standardize the relative abundance results obtained from 16S profiling across different studies, and 3) the lack of a user-friendly bioinformatics pipeline that can be operated by clinicians without extensive bioinformatics knowledge.

The MYcrobiota platform is the result of a close collaboration with Streeklab Haarlem~\cite{url-streeklab}, a microbial diagnostics lab servicing a large number of GPs and hospitals in the region. In order to facilitate the use of 16S rRNA sequencing in a diagnostic setting, several enhancements to standard procedure were required, both in the wet lab and the bioinformatics pipelines to overcome the aforementioned obstacles.

The MYcrobiota platform utilizes the micelle PCR (micPCR) method~\cite{boers2015micelle,boers2017novel}. In this approach, PCR amplifications of each template 16S template sequence occurs in a physically distinct reaction environment called a micelle, thus greatly reducing the generation of hybrid sequences known as chimeras. This approach has the additional benefit of allowing for the absolute quantification of abundance by eliminating PCR competition induced bias. To further improve the accuracy of the results, each sample was sequenced in triplicate and averaged in order to eliminate any remaining quantification bias in the micPCR protocol. A further correction is performed with a negative extraction control sample sequenced with every batch. Finally, and internal calibrator (IC) was used to enable quantification of each resulting OTU in terms of the number of gene copies rather than relative abundances. This IC consisted of a known quantity of a bacterium not present in the natural microbial flora under investigation added to the samples before PCR amplification.

Results were validated through the analysis of 47 clinical samples obtained from patients presenting with a variety of damaged skin conditions, and results were compared to the culture-based methods currently employed for routine clinical microbial diagnostics. The results showed that the vast majority (>95\%) of genera detected by routine culturing were also detected by the MYcrobiota platform. Conversely, the majority of bacterial taxa detected by MYcrobiota were not identified by culture, and many of these additional genera detected were obligate anaerobes consistent with previous studies \cite{TODO}, and included potential pathogens such as \emph{Kingella} not detected in routine culture.

The universality of the MYcrobiota pipeline has been subsequently demonstrated through its application to environmental studies in drinking water distribution systems~\cite{boers2018monitoring} and in a clinical setting involving patients presenting with suspected septic arthritis~\cite{boers2018detection}.

It must be noted however, that certain limitations to the MYcrobiota remain, and further development and extensive clinical validation studies are required before introduction into routine diagnostics. For example, the current methodologies lack the discriminative power to differentiate to the species taxonomic level, which is often essential for clinical diagnostics. However, results could be supplemented with species-specific PCRs. Alternatively, relatively simple alterations to the current MYcrobiota platform could accommodate approaches such as the sequencing of multiple hypervariable regions or full-gene 16S rRNA, as well as other potential genetic markers such as \emph{rpoB}~\cite{adekambi2009rpob}, \emph{gyrB}~\cite{yanamoto1995pcr}, the ITS region~\cite{schoch20012nuclear}, or one of many other potential markers capable of differentiation of prokaryotes at the species taxonomic level \cite{lab2016marker,sabat2017targeted}.

In conclusion, the development of the MYcrobiota platform paves the way for the introduction of culture-free quantitative microbiota profiling methods into clinical diagnostic laboratories. It is capable of providing a highly accurate and comprehensive profile of the microbial composition of clinical samples, even at low biomass, and may provide clinicians with valuable information on potential pathogens not (easily) provided by the standard culture-based methods. Alternatively, the culture-negative status of clinical samples may be confirmed by the absence of 16S rRNA gene copies in the MYcrobiota results.

\subsubsection{The Informatics}

\textbf{Tools}. As a first step to creating the MYcrobiota platform, we incorporated the full suite of 125+ mothur tools~\cite{schloss} into Galaxy, as well as the Krona~\cite{ondov2015krona} tool and Phinch~\cite{bik2014phinch} display application for visualisation of tools. To facilitate the interoperability of these tools and others already available in Galaxy, we also created some file format conversion tools to function as the \emph{glue} between steps and facilitate the interoperability with existing downstream tools through the support of widely used file formats such as the BIOM format~\cite{mcdonald2012biological}.


\textbf{Workflows}. In order to optimize utility of the tools, several standard pipelines were provided in Galaxy, based on available standard operating procedures (SOPs) defined in the research community. However, for use as clinical pipelines, further customizations were required, for instance to support the experimental setups utilizing such methods as replicates, negative extraction controls, as well as additional alterations necessitated by the use of micPCR protocol, such as the use of an IC for quantification. To accommodate these custom requirements, the standard pipelines had to be augmented with additional components and custom parameter settings, arrived at through a lengthy cycle of testing and adjustment, followed by clinical validation. In order to facilitate scaling of these analysis, the pipelines utilize Galaxy collections, enabling analysis sizes from single sample to tens of thousands of samples.

\textbf{Visualisation and Reporting}. The MYcrobiota pipelines generate hundreds of files per run per sample, so a tailor-made web report was configured using the iReport tool described in Chapter \ref{chapter:ireport} to aid clinicians in the interpretation of results. This report included several integrations with external resources such as BLAST and prokaryotic databases.

\textbf{Open science and bioinformatics best practices}. In order to optimize the utility of the tools and pipelines both now and in the future, all components of the MYcrobiota are open-source and publicly available on GitHub, and under testing using the Travis continuous integration platform. The advantages of this approach are illustrated by the fact that since its release, the tools have received numerous updates and bug fixes from members of the Galaxy community, relieving the maintenance burden for us as original authors and keeping the tools relevant as the underlying mothur components evolve.

\textbf{Training}. Galaxy training materials were developed to facilitate the dissemination of the MYcrobiota tools and pipelines. These were integrated into the training infrastructure developed in Chapter~\ref{chapter:training}, and have since been used in numerous workshops by a variety of instructors in the Galaxy Training Network, as well as for self-study by individual learners online. Due to the feedback mechanisms built into the Galaxy training framework, learner feedback is collected, and we have received and implemented many suggestions for improvements, enabling incremental development and refinement of the materials.

\section{Future Perspectives: Futuromics}

The field of bioinformatics is in a constant state of flux, with data being generated at an exponential rate, and new sequencing technologies ever on the horizon promising greater biological insight. As such, many of the currently used tools, algorithms and file formats will evolve or be replaced by new ones, and the challenge will be to create and maintain the IT infrastructure required to support these novel tools and techniques and to scale with the exponential rate of data analysis in this era of \emph{big data}.

\subsection{The Bio}
% long reads
As sequencing technologies continue to evolve, so will the entire ecosystem of bioinformatics tools and algorithms surrounding them. Microbiota profiling is currently usually limited to a single hypervariable region of the 16S rRNA gene, but as long-read technologies such as Nanopore mature, whole-gene sequencing of the 16S gene may increase resolution and allow for taxonomic determination down to the species level, which will greatly increase its utility in clinical applications. Long-read technologies are equally promising in cancer analyses, where they have the potential to span large-scale structural variants and greatly aid in the reconstruction of highly rearranged tumour genomes or even chromosomes affected by chromothripsis. Currently such technologies are limited due to a high error rate as compared to traditional short-read sequencing technologies, but these error rates are quickly falling to acceptable levels. Another limitation is the limited availability of tools in the community aimed at the use of such relatively novel technologies, but this is remedied my the simple passage of time, and as the technologies improve, so does the incentive to develop tools capable of analysing the resulting data.

% single cell
Similarly, the advent of single-cell sequencing technologies will greatly aid cancer genetics research by providing a more accurate view of the state of a single cell within a tumour, and allowing for the characterization of clonality and temporal evolution of the tumour by sampling in physically and temporally distinct locations.


\subsection{The Informatics}
% big data
Data are being generated at an exponential rate, with estimates placing the storage capacity needed for human genomes alone as high as 40 exabytes in 2025~\cite{stephens2015}. But storage space is only a small part of the story. The far greater challenge will lie in the data stewardship; the efficient handling of this data in a way that it can be easily searched, accessed and shared within the worldwide scientific community. Initiatives such as the FAIR data project~\cite{wilkinson2016fair} are aimed at providing the necessary best practice guidelines needed for the efficient management of the genomic big data era.

% Galaxy for the clinic? (reporting, frontends)

% Open science! Open science! Open science!
With the large amounts of data being generated, so will the amount of new (biological) knowledge obtained from this data. Ideally, science should be a collective pursuit, and the sadly still-pervasive attitudes inhibiting the sharing of data for often irrational fears of being scooped should be replaced with attitudes geared toward complete transparency and adherence to open science principles~\cite{TODO}. This will require changes in attitudes not only in individual researches, but also academia as a whole, with a vital role for scientific journals. This shift is already underway, as evidenced by the observation that academic journals increasingly are making submission of data to public databases a prerequisite for publication. Not only will this increase the reproducibility of results, one of the cornerstones of scientific research, but it will also increase the accuracy of results by allowing for more thorough peer review. Furthermore, it will increase the rate of knowledge acquisition by allowing for the integration of data from research institutes around the world. Similarly, analysis software and tools also greatly benefit from this open science attitude by enabling the entire global community to serve as potential source code reviewers and contributors, which will benefit the code quality in ways a single developer in one lab can never hope to compete with.


\begin{center}
\emph{â€œNo man is an island, entire of itself; every man is a piece of the continent, a part of the main.} - John Donne
\end{center}

\bibliographystyle{ieeetr}
\bibliography{references}

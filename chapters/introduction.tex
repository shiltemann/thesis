\begin{savequote}[75mm]
"This job is a great scientific adventure. But it's also a great human adventure. Mankind has made giant steps forward. However, what we know is really very, very little compared to what we still have to know."
\qauthor{Fabiola Gianotti}
\end{savequote}



\chapter{Introduction}
\label{introduction}
 % start numberings at 0 because (computer) science
\setcounter{figure}{-1}
\setcounter{table}{-1}
\setcounter{section}{-1}
\setcounter{NAT@ctr}{-1}

\setlength\parindent{0pt}

\begin{justify}

\section{The Source Code of Life}

DNA. The blueprint of life. These long double-stranded helical molecules are present in all living cells on earth\footnote{As far as currently known. Some viruses contain only RNA, but these are often not considered "alive".} and encode the proteins which drive the functioning, structure and replication of the cells and tissues that make up an organism. In many ways, DNA is also analogous to computer code; any computer program, no matter how complex, can be described as a long series of just two characters, \verb+0+ and \verb+1+, known as \emph{bits}. Knowing the sequence of these bits and, crucially, the details about how they are being interpreted by the machine on which they are executed, enables us to understand and predict the functions they encode. In much the same way, DNA uses just 4 different elements, called \emph{bases} or \emph{nucleotides}, to encode its blueprint for the cell. These 4 building blocks are adenine, cytosine, guanine and thymine, usually referred to simply by their first letters, \verb+A+, \verb+C+, \verb+T+ and \verb+G+. These bases combine together in pairs (\emph{base pairs}), with \verb+A+ always matching to \verb+T+ and \verb+C+ being complementary to \verb+G+ along a sugar phosphate backbone in their double-helix configuration (Figure \ref{fig:dnastructure}).

\end{justify}


\begin{wrapfigure}{r}{210pt}
%\begin{figure}[h!]
    \centering
    \includegraphics[width=200pt]{chapters/images/introduction/dna-structure.png}
    \caption{Structure of DNA}
    \label{fig:dnastructure}
%\end{figure}
\end{wrapfigure}

In humans, DNA is organised into 23 pairs of chromosomes, with a total length of over 3 billion base pairs. About 1\% of this DNA make up \emph{genes}; stretches of DNA which directly encode protein molecules. When these genes are expressed, the gene sequence is transcribed to a messenger molecule (\emph{mRNA}) which is subsequently translated into a protein molecule. The function of the remaining 99\% of the genome long remained a mystery, and was even referred to as \emph{junk DNA}. More recent studies have revealed that many of these stretches of non-coding DNA play an important role in the regulation of the expression of genes, stimulating or prohibiting the translation of genes to proteins and thereby influencing the functioning of the cell. For any two individuals, the vast majority of their DNA sequence will be identical, but small variations in the remaining locations of the genome are what make each of us unique. But it is also what can make us sick. Studying this natural variation in the genome sequence helps us unravel the mechanisms of life and disease.

\begin{justify}
\subsection{A Brief History of Genome Sequencing}
Friedrich Miescher was the first to isolate DNA molecules (which he termed \emph{nuclein}) in 1869. However, the molecule remained relatively understudied until Rosalind Franklin used X-ray crystallography to inspect the molecule's structure \cite{elkin2003rosalind}. Watson and Crick famously used this data to postulate the double-helix model of DNA in 1953 \cite{watson1953molecular}. This model illustrated for the first time that DNA molecules were ideally suited for replication, and solidified the idea that DNA, and not protein as was previously thought, was the primary carrier of hereditary information. Simultaneously, Frederick Sanger had developed techniques to sequence proteins, and later RNA and DNA molecules. This was the start of the \emph{first generation} of genome sequencing, and the first full gene was sequenced in 1972, followed by the first full genome in 1976 (bacteriophage Ï•X174, 5386 nucleotides long) \cite{sanger1977dna}. These early techniques were only suitable for relatively small sections of DNA, and more technological advances would be required before larger genomes could be fully sequenced.

In 1990, the Human Genome Project \cite{olson1993human} set out to sequence the entire 3.2-billion-basepair-long human genome, an effort culminating in 2003 with the publication of the first human \textit{reference genome} \cite{international2004finishing}. Not only did this provide invaluable insights into human genetics, but it also paved the way for the next era in genetic research; something which would completely transform the field of genetic research. Over the next several years, \emph{next-generation} massively parallel sequencers were developed by companies like Roche454 and Illumina, dramatically cutting the cost and time required to sequence a human genome, and for the first time demonstrated its potential utility in clinical and diagnostic settings. Through sustained technological advancements over the following years, these costs continued to decrease at exponential rates - outstripping even the pace predicted by Moore's law (\hyperref[fig:seqcost]{Fig. \ref{fig:seqcost}}) - and the long dreamed-about \textit{\$1,000 dollar genome} \cite{thousanddollargenome} \cite{sequencingcostsNHGRI} has now become a reality.

\begin{figure}[h!]
    \centering
    \includegraphics[width=300pt]{chapters/images/Historic_cost_of_sequencing_a_human_genome.png}
    \caption{The cost of sequencing a human-sized genome over time. Data from the NHGRI Genome Sequencing Program (GSP) }
    \label{fig:seqcost}
\end{figure}

Shortly after the publication of the human genome, high-throughput techniques emerged to also sequence the transcriptome (RNA-Seq), allowing for the identification and quantification of gene transcripts, and providing information about post-transcriptional mutations and other complexities such as alternative splicing \cite{wang2009rna}. Similarly, the study of epigenetics is revealing that non-coding DNA, far from the once-termed \textit{"junk DNA"}, is part of an intricate and complex regulatory system controlling the expression of genes \cite{zuckerkandl2007combinatorial}.

Now we are now moving into the era of \emph{third-generation} sequencing, where single-cell \cite{gawad2016single}, long-read \cite{koren2015one}, and often real-time sequencing \cite{flusberg2010direct} are allowing for ever more accurate determination of nucleotide sequence, providing increased resolution even in highly diverse and complex samples such as cancer or metagenomic samples. All these technological advances have led to a deluge of data that must be managed and analysed, typically by bioinformaticians.


\section{The Bioinformatics Challenge}

With huge amounts of data now being generated at relatively low cost \cite{chen2014big}, and compute resources being available even on moderate budgets, the challenge in genomic research has shifted from the sequencing technologies to the analysis and interpretation of these big and highly complex datasets, and the development of the software required in order to gain new understanding of the underlying biological systems. This poses a significant challenge, both technologically and scientifically.


\subsection{A jigsaw genome}
Current sequencing techniques cannot simply read all the nucleotides of an entire chromosome at once; instead, DNA molecules are cut into tiny pieces of a few hundred bases, and each of those small fragments is sequenced. The full genome must then be reconstructed from these short sequences reads. Think of this as solving a jigsaw puzzle with billions of pieces. To complicate matters further, the nucleotides determined by the sequencer (\emph{base calls}) are not always correct. Furthermore, we typically don't sequence the DNA from just one cell, but combine material from a large number of cells together and will have a lot of overlapping pieces. And of course the genome itself contains complications such as repetitive regions, that complicate the reconstruction further. In the case of human DNA, we have the reference genome as a guide, and can use this as a scaffold to map each of the reads to. For other organisms however, such a reference genome does not exist, and the short reads must be assembled \emph{de-novo} to reconstruct the genome. Think of this as solving the jigsaw puzzle with billions of pieces, but without the picture on the box to guide you.


\subsection{Ever-changing landscape}
Sequencing technologies are evolving at a staggering pace, and with them, so must the software tools capable of analysing the data. Therefore, new tools are developed continually and existing tools must be updated regularly to remain relevant. As a consequence, there typically simply is not enough time for community consensus and data and analysis standards to emerge organically before the advent of new technologies make them obsolete. Because of this, there usually are a multitude of tools available for any given task, each with their own set of advantages and disadvantages, and the challenge is to find the right tool for the particular situation, and knowing when to use existing tools and when the development of novel tools is in order.


\subsection{Standardisation}
A typical bioinformatics analysis will consist of many different tools, chained together into a \emph{pipeline}. In order for data to pass smoothly from one tool to the next, it is important to agree on standard file formats. Unfortunately, in the absence of an authoritative body to prescribe such standards, these not exist in most cases, and where they do, they often allow for a large degree of flexibility. As a result, files cannot always be passed seamlessly from one tool in the pipeline to the next, and custom file transformation steps are often required as a \emph{glue} of sorts between the different analysis steps.


\subsection{Data Storage}
 \begin{comment} https://qumulo.com/blog/genomic-sequencing-qf2-storage/ \end{comment}
It has been predicted that genomics may soon overtake other \emph{Big Data} domains such as astronomy and video-hosting platforms such as Youtube in terms of yearly data generation \cite{Stephens2015}. Handling these vast amounts of data is a challenge many bioinformaticians face. Not only must the data be stored somewhere where it can be easily accessed, it must also be organised, backed up, and shared in an efficient way. Any storage solution must be able to scale to the exponential trend of growth both in size and number of files. Furthermore, data storage solutions must comply with privacy and security requirements, which is especially important in the case of human genetic data.


\subsection{The Specialist Bioinformatician}
All of these challenges have resulted in bioinformatics becoming a highly specialized field, which in itself poses a new challenge given the observation that the domain knowledge (biology) and the informatics know-how more and more often do not reside in a single individual, and the interpretation of the data cannot always be done by the person performing the data analysis and vice versa. Instead, close communication between the two fields is required, and ideally the domain experts should be empowered to perform their own day-to-day data analyses without the need of a bioinformatician.


% some (partial) solutions to the above challenges, spoilers: open software, open science, open everything
\section{Bioinformatics Best Practices}

In order to optimally deal with all the challenges in bioinformatics, adhering to a set of best-practices guidelines may be beneficial. Such best practices may pertain to:

\begin{enumerate}
    \itemsep-0.5em
    \item Accessibility of tools and data %(usability) of tools (documentation, open source, github etc, galaxy, galaxy, dependency management, docker)
    \item Reproducibility %(conda, docker, galaxy, version control, code notebooks)
    \item Interoperability %(data formats, common frameworks, generic, allow for packaging)
    \item Maintainability of software %(testing, continuous integration, community buildingi, scalability)
    \item Visualization and reporting
    %\item Data management and sharing %(FAIR data, LIMS, PIDs)
\end{enumerate}


\subsubsection{Accessibility}
Most bioinformatics tools are command-line UNIX \cite{url-unix} tools, and biologists are not typically trained in the use of such. Even for the experienced bioinformatician, running some of these tools can be a challenge due to lack of documentation or quality of the tool. Ideally, once a tool or pipeline has been validated, the analysis can be run by the domain expert, i.e. the research scientist responsible for the interpretation of the analysis results, without being reliant on the support of a bioinformatician at every step \cite{kumar2007bioinformatics}.

Creating user-friendly software is not a trivial task. Application linking (also referred to as wrapping), can ease this burden for the tool developer. In such an approach, existing user-friendly interfaces host third-party software packages -at minimum effort to the developer of the hosted software- and thereby offer a layer of abstraction to the end-user that shields them from the implementation details of the tool and provides a uniform usage paradigm for all tools, regardless of their differences behind the scenes. Examples of such hosting frameworks in the context of bioinformatics are Galaxy \cite{giardine2005galaxy,goecks2010galaxy} and Taverna \cite{oinn2004taverna}.

Accessibility also includes the creation of high-quality documentation of tools, both aimed at developers and end-users, and ideally some form of training manual to educate users in the proper use of tool and to warn about possible pitfalls and biases.

\subsubsection{Reproducibility}

A cornerstone of the scientific method is reproducibility of results. Experiments should be described in sufficient detail to allow for their reproduction and independent verification by fellow scientists. In reality however, this remains a big challenge, and many publications can not be reliably reproduced based on the information provided in publication \cite{baggerly2009reproducible,kim2018reproducibility}. In many cases this holds not only for the wet lab experiment, but also for the in-silico downstream data analysis. Reproducing bioinformatics analyses often becomes an exercise in \textit{forensic bioinformatics}; trying to piece together the exact procedure used through trial and error and educated guessing rather than by the information provided by the original researchers. In part this is caused by journals not having clear submission guidelines for bioinformatics pipeline descriptions as they do for the physical samples. The latter are commonly required to be submitted to biobanks before submission, and the raw datasets generated from them to be stored in online repositories. No such requirements are generally imposed on the downstream analysis, or when they are, these requirements are not strict enough to ensure true reproducibility. This is not without consequences; in some cases this has lead to clinical trials being started based on incorrect conclusions not revealed during peer review due to lack of reproducibility \cite{baggerly2009reproducible}.

\begin{comment}
Many journals require authors to submit datasets to public repositories in order to allow other scientists to reproduce their findings. With bioinformatics now such a fundamental part of any publication, a clear description of the analysis pipeline is also required for true reproducibility, but the majority of scientific publication do not contain all the information required to reproduce the in-silico experiment [cite]. Making analysis software open-source and adhering to a set of bioinformatics best-practices can improve the reproducibility of bioinformatics analysis pipelines.
\end{comment}

% dependency management
While reproducibility is a high priority in the scientific community, this is not true for software developers in general, and bioinformatics pipelines are often dependent on a combination of scientific and more general-purpose components. However, many software packages will simply be removed once an update has been made available, hindering reproducibility of all but the most recent analyses. This is where package managers such as Conda \cite{conda} or GNU Guix \cite{courtes2013functional} offer significant improvement, by allowing any historic versions of tools to be installed at all times. This aims to ensure that the full stack of software and dependencies obtained during installation of the pipeline will yield identical results when performed today as it will a year from now or 5 years from now.

% provenance
Once the correct set of software is obtained, the pipeline \emph{provenance} must also be captured; metadata describing how the tools were applied to the data in the analysis run. This includes the sequence and interplay of different tool executions, with the full set of parameter used and the input and reference data used at every step. Keeping a lab journal of the in-silico experiment is a good start, but is too error-prone as a manual process. Projects such as Jupyter Notebooks \cite{kluyver2016jupyter} for Python, or Sweave \cite{leisch2002sweave} and KnitR \cite{xie2014knitr} for R, allow for the mixing of text and code to create a kind of interactive journal article. Calculations can be embedded within the text, and these calculations may be examined and rerun or adjusted with ease by the reader. A drawback of this approach comes when tools require a lot of resources or are not all written in the same language, as is typically the case for bioinformatics analyses. Workflow platforms such as Galaxy automatically keep track of provenance for the user and have the advantage of supporting a wide range of tools.

\begin{comment}
"forensic bioinformatics" -> have to figure out by trial and error what was done

https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0377-6
https://www.biostars.org/p/52561/
https://www.nature.com/naturejobs/science/articles/10.1038/nj7396-137a
https://www.nature.com/nm/journal/v13/n11/full/nm1107-1276b.html
\end{comment}

\subsection{Interoperability}

% standards
The aforementioned lack of data format standards hinders the interoperability of tools. Even in cases where file formats are somewhat standardized, variations in the exact implementations may still require careful consideration within a pipeline. Consider for example the FASTQ format; this is a widely used and relatively simple format, it consists of 4 lines per sequence read, but the line containing quality score, is the source of some divergence in the format. A range of ASCII characters is used to encode numerical values indicating a PHRED-like quality score of the base call, but the range and start position of this character set comes in different flavours. Furthermore, because these ranges overlap, it is not always possible to deduce which convention was used unless some quality encoding are present in the file that only appear in one of these conventions. It is of crucial importance to know the encoding scheme used for any datasets entering your pipeline or low-quality reads could be mistaken for high-quality reads and vice versa. Ideally such specifications would be part of the data format itself, e.g. as a file header, but for FASTQ and many other formats this is not the case.

Similar widespread variations in standard formats include chromosomal location (0-based or 1-based numbering, open or closed) and chromosome names (with or without a \verb+chr+ prefix) or gene names (many different naming schemes). These issues are not hard to deal with on a conceptual level, but in practise can easily lead to inaccurate results if not carefully taken into account by the creator of the pipeline.

Such concerns also exist on the more purely informatics side of bioinformatics as well. Take the example of bash scripts, many OS-dependent syntax variations can hinder interoperability, but by taking care to comply with POSIX standards, such concerns can be mitigated.

% technical interoperability
Going beyond file formats, interoperability of different tools may be hindered due to the fact that they are often written in different programming languages and/or designed for different operating systems. Fortunately, tool developers can undertake steps to make their tools more interoperable. Package managers such as Bioconda \cite{gruning2017bioconda} will compile software from its source for a variety of different operating systems and simplify installation for end users.

% documentation!
Finally, and vitally, precise documentation of any assumptions made in the code is indispensable, and making source code open further facilitates this transparency and interoperability.


\subsection{Maintainability}
Since scientific research depends increasingly on software, decreasing the burden of tool maintenance is valuable and worthwhile pursuit. Furthermore, many tools are written by small research groups or single individuals who are simply unable to perform the necessary maintenance without support. By making tools open-source, the entire bioinformatics community is able to step up as co-developer, allowing them to discover bugs and contribute fixes or enhancements. Code sharing platforms like GitHub \cite{url-github} and BitBucket \cite{url-bitbucket} facilitate this community-driven approach to software maintenance.

Using code versioning such frameworks such as git \cite{url-git} or mercurial \cite{url-mercurial} ensures precise tracking of changes and enable collaborative tool development. Incorporating tests at every phase of development can further decrease the maintenance effort. Unit tests ensure that small code modules show expected behaviour at all times. Functional tests ensure that these different units of code always yield the desired result when working in conjunction. Code quality checks can help streamline the code itself and increase readability, benefiting future development. Continuous integration is the paradigm whereby changes to the mainline code are incorporated incrementally and continually, and thoroughly reviewed and tested at every stage, thereby decreasing the maintenance burden of the software. Code sharing platform such as GitHub and BitBucket offer continuous integration frameworks as part of their service.

\subsection{Visualisation and Reporting}

As scientists become increasingly reliant on large and complex computational analyses in their research \cite{chen2014big}, the final analysis result datasets become similarly complex and have often grown beyond the realm of what can be manually viewed and interpreted, both in terms of the number of files and their sizes, as in terms of complexity. Analysis results therefore require summation and visualisation and must be presented to the domain expert in a comprehensible and accessible manner. Such a report should also contain detailed descriptions of the methods used and assumptions made and citations to any third-party tools employed, as an understanding of these factor can assist in interpretation \cite{kumar2007bioinformatics}.

For genomics data, visualisation tools such as Circos \cite{circos} and MultiQC \cite{ewels2016multiqc} enable the integration of various output datasets, often in the order of millions of lines each, to be summarized in a single image or report. While some resolution may be lost in such visualisations, they do enable the easy identification of areas of interest and guide the interpretation by pointing the domain experts in the direction of further inspection.


\section{Bioinformatics for Everybody}

In order to empower domain experts to analyze their own data, analyses must be made available in a user-friendly way, not requiring any expertise in programming, and secondly, scientists must be trained to use and understand the tools and pipelines they employ in their analyses.

\subsection{The Galaxy Project}
The Galaxy project \cite{afgan2016galaxy} provides a user-friendly graphical interface to command-line tools, bringing the data analysis to the domain experts equipped to perform the interpretation of the results. It also facilitates many of the best-practice bioinformatics guidelines outlined in the previous section; with its heavy focus on accessibility and reproducibility, Galaxy is an potent framework for creating high-quality bioinformatics pipelines. Galaxy keeps track of the full analysis provenance, manages tool dependencies with Bioconda \cite{gruning2017bioconda}, has built-in visualisations, and is accessible by end users with nothing more than a web browser. For developers, Galaxy provides a convenient framework to package tools to make these available for researchers throughout the global community. The Galaxy tool shed \cite{url-toolshed} is the central repository of tools that anybody may contribute to, and contains over 6500 tools.

\subsection{Training}
With -omics research becoming increasingly computational in nature, and many research groups not having access to a dedicated bioinformatician, there is a great need for high-quality bioinformatics training to ensure that the domain experts who interpret the results of data analyses are optimally equipped to do so. Surveys confirm this need for bioinformatic training; the majority of researchers (>95\%) work with or plan to work with large datasets, but most (>65\%) possess only minimal bioinformatics skills and are not comfortable with statistical analyses \cite{larcombe2017elixir}, \cite{williams2017vision}. Demand for training currently greatly exceeds the supply \cite{attwood2017global}. In a recent survey \cite{survey2013embl} over 60\% of biologists expressed a need for more training while only 5\% called for more computing power. Thus one can assume that the true bottleneck of the current data deluge is not storage or processing power, but the knowledge and skills to utilize existing resources.

With its focus on accessibility and user-friendliness, the Galaxy platform is also an ideal environment for teaching. Trainees are shielded from the minutiae of the implementation details of the underlying tools, and need nothing more than a browser to execute the tutorials. Because Galaxy can host any command-line tools and the tools available in the tool shed cover a wide range of topics, creation and maintenance of a set of training materials must be a collaborative community effort, with content being created by a large number of people with expertise in the various topics.


% Use case 1: Prostate Cancer

\newpage
\thispagestyle{empty}
\begin{center}
\vspace{2cm}
\begin{minipage}{6in}
\tikz[remember picture,overlay]
\node[opacity=0.8,inner sep=0pt] at (current page.center){
    %\includegraphics[width=\paperwidth,height=\paperheight]{chapters/images/background-texture-blue.jpg} %higherquality image but too big for overleaf
    \includegraphics[width=\paperwidth,height=\paperheight]{chapters/images/background-texture-blue-reduced.jpg}
};
\sc
\begin{center}

\color{white}{
\Large The Jigsaw Genome \normalsize
\vspace{2cm}


I like puzzles. Any type of puzzle. I always have. If I see a puzzle or a problem I have to solve it. I think that is what makes cancer such a fascinating topic for me.

\vspace*{0.5cm}

Imagine you are given a jigsaw puzzle. Now instead of a few hundred pieces, there are several billion pieces. The picture on the box is not a picture of the puzzle inside the box, it is just a somewhat similar image. Oh, and did I mention there are a whole bunch of pieces missing? And that many pieces are duplicated? Some pieces don't even belong in our box, but come from a completely different puzzle. On top of that your little sister has spilled paint over some of the pieces so those can't be trusted to contribute to the image. And instead of one single puzzle, the box contains several, they are all variations of the image on the box, but you have no idea how many different puzzles the box contains. Sound challenging? This is the problem we are solving whenever we sequence a cancer genome.

\vspace*{0.5cm}
\textbf{[Metaphor key]} \\
\textit{puzzle pieces} = sequence reads \\
\textit{picture on box} = reference genome \\
\textit{missing pieces} = hard-to-sequence areas \\
\textit{other puzzles} = contamination \\
\textit{painted pieces} = sequencing errors \\
\textit{multiple puzzles in box} = clonality
}

\end{center}
\end{minipage}
\end{center}



\newpage
\section{Use case 1: Prostate Cancer}

\epigraph{3in}{The time has come in America when the same kind of concentrated effort that split the atom and took man to the moon should be turned toward conquering this dread disease.}{President Richard Nixon}

On December 23, 1971, President Richard Nixon, buoyed by recent technical feats such as the moon landing, signed into law the National Cancer Act, thereby declaring a war on cancer. Today, more than 45 years later, that war is still being waged in full force. While great advancements have been made towards this goal, some of the initial optimism has been quelled by discoveries of the great complexity and heterogeneity underlying cancer.


\subsection{The Hallmarks of Cancer}

Tumor cells evolve from normal cells through the acquisition and accumulation of mutations. The human body has mechanisms in place to repair or dispose of damaged cells and to prevent runaway cell division, so in order for a tumour cell to survive and thrive it needs to acquire changes that provide it with advantages for proliferation and evasion of the cell's defense mechanisms. Evidence suggests this transformation from healthy cells into malignant cells follows a strikingly similar path across all different tumour types \cite{}. A cell's acquired abilities that drive tumour progression are know as the \emph{hallmarks of cancer} and consist of the following six characteristics:

\begin{itemize}
    \itemsep-0.5em
    \item self-sufficiency in growth signals
    \item insensitivity to anti-growth signals
    \item evasion of programmed cell death
    \item limitless replicative potential
    \item sustained angiogenesis
    \item tissue invasion and metastasis
\end{itemize}

Each of these steps overcomes one of the body's anti-cancer defense mechanisms. This evolution into malignancy is an almost Darwinian process where the mutations acquired are random, but those cells that have gained mutations which are advantageous for survival will be able to replicate and thrive and accumulate further mutations. Distinguishing the mutations that impart a strategic advantage and thereby \emph{drive} a tumour's progression, from the often huge number of less harmful \emph{passenger} mutations accumulated over the lifetime of a cancer cell is crucial to our understanding of cancer progression \cite{stratton2009cancer}. The optimal course of treatment for a patient often depends on the mutations present and how the cell functions are subsequently impacted by those mutations. %However, many different mutations may lead to the same disruptions of key pathways, therefore we must evaluate mutations not just at the DNA level but in the broader context of their functional impact on the cell's internal processes.



\subsection{Cancer's Complexities}

Determining the exact genetic sequence of healthy individuals is already quite a challenging endeavor; trying to extend this to cancer genomes takes this challenge to the extreme. There are several complexities present in cancer genomes that make accurate determination of the genetic changes and their downstream impacts a difficult task. In the following sections we will discuss some of these complexities and explore the biological and informatics challenges they pose.

\subsubsection{The Bio}
\paragraph{Small variants} comprise the simplest class of mutations; those consisting of alterations of just a handful of bases, for instance the \emph{substitution}, \emph{deletion} or \emph{insertion} of one or more nucleotides.

The impact of such mutations depends on where in the genome they occur. Single nucleotide variants (SNVs) in exonic regions can range from having no effect on the resulting protein (silent), to changing an amino acid in the protein to a different amino acid (missense mutations), to changing a codon into a stop codon (nonsense mutation) which nearly always results in a nonfunctional protein. If this happens in a protein that is vital to the functioning of the cell this can have disastrous consequences <some examples of SNVs causing serious problems/phenotypes>

While variants within the coding sequence are most likely to have an impact on cell health, small variants \emph{outside} the coding sequence can also have drastic impact on health; for example, 70\% of melanomas exhibit a point mutation in one of two positions in the promoter region of TERT (Horn et al. 2013; Huang et al. 2013), suggesting such somatic point mutations in regulatory regions may play a role in tumorigenesis.

\paragraph{Structural variations (SVs)} are larger-scale mutations involving rearrangement of segments of DNA of more than roughly 50 bp. In some cases, these rearrangements can cause (parts of) two genes that are usually separated by some distance on the genome to come together to form new hybrid genes, which may be transcribed into fusion proteins, and which have a potentially disastrous effect on cell processes. The text book example of such a gene fusion is the so-called Philadelphia fusion frequently observed in leukemia \cite{TODO}. In this Philadelphia chromosome, a translocation between chromosomes 9 and 22 leads to a fusion of the BCR and ABL1 genes. This fusion gene is expressed, and the aberrant fusion protein causes disruptions to key signalling pathways governing the cell cycle, causing the cells to divide uncontrollably and thereby drive tumor progression. Accurate detection of such structural variations is crucial, as they may serve as diagnostic markers \cite{nowell1960chromosome,nowell1961chromosome} or even therapeutic targets \cite{druker2001activity, druker2001efficacy}.

dSVs often occur outside coding regions, but this does not diminish their potential for great impact, for instance by disruption of the regulatory mechanisms of tumour suppressor genes or oncogenes \cite{TODO}. These types of SVs not involving coding regions directly, may only be detected through whole-genome sequencing. In rare cases, \emph{chromothripsis} may occur; as opposed to the more common gradual acquisition of mutations over time, chromothripsis is a shattering of (part of) a chromosome in a single catastrophic event, and the subsequent imprecise stitching back together of the chromosome by the cell's repair mechanisms. This results in a cluster of thousands of SVs in a confined genomic region. Chromothripsis occurs in about approximately 2\%-3\% of cancer genomes, with a significantly increased rate incidence in some cancer types \cite{luijten2018}. Identification and reconstruction of such highly rearranged cancer genomes in order to predict their effects remains a challenge \cite{yang2016chromothripsis,govind2014}.

% good review paper for chromothripsis: luijten2018

%Numerous fusion genes have been found to play a role in tumorigenesis \cite{TODO},
%in prostate cancer the TMPRSS2-ERG fusion is present in roughly 40\% of tumours, \cite{TODO}

\paragraph{Clonality} refers to the phenomenon that cells in different physical areas in a tumour may be genetically very distinct. As a cell obtains a mutation that is beneficial to proliferation, it may instigate a new cluster, or \textit{clone}, of genetically similar cells while in other areas of the tumour cells may follow a different evolutionary path and create genetically different sub-clones. When sequencing a tumour, DNA from different cells -and thus potentially very different genomes- is mixed together and sequenced as one. Reconstructing the different clonal genomes from this data is immensely challenging.

\paragraph{Further complexities} in the analysis of cancer genomes include the temporally evolving nature of tumours; performing the same sequencing experiment on the same tumour at different points in time will often yield a very different picture as the composition of the tumour and the acquired mutations will have changed. This further complicates the process of comparing different samples and elucidating key characteristics that have the potential to aid in the diagnosis and treatment of patients. Furthermore, studies in epigenetics have shown that tumorigenesis is not solely driven by mutations in nucleotide sequence, but secondary alterations such as DNA methylation may have an equal if not greater impact \cite{pacchierotti2015environmental}.

\subsubsection{The Informatics}

All these complexities in the biology of cancer genomes translate to complexities of the downstream informatics analysis pipelines.

Mutations in DNA are described relative to a \emph{reference genome}, and comparing variants across different samples is inherently challenging for a variety of reasons. Observed differences between a sample and the reference genome could indicate a true biological variant, or be the result of a sequencing or mapping error, and separating the true variants from the noise is a task that each variant calling tool approaches differently. As a result, different variant callers often have poor overlap \cite{orawe2013, hwang2015systematic}. This poor concordance combined with the observation that there are often various different but equally valid ways of describing a variant (Figure \ref{fig:variant-multiple-representations}), hinders the comparison of variant between different samples and studies \cite{zook2014integrating}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=400pt]{chapters/images/variant-representation-all.png}
    \begin{comment}
        image in google drive https://docs.google.com/drawings/d/1WWmzW6uWtJ7HZ5WVgtg_oAMA_3E_569M4Ikjz4Ecjic/edit
    \end{comment}
    \caption{Complex variants can be represented in multiple ways. Four different aligners (Novoalign, Ssaha2, BWA, Complete Genomics) treat the same variant in vastly different ways, which leads to such differing sets of variant descriptions in the VCF files that it is no longer apparent that these variants in fact describe the same observed sequence.}
    \label{fig:variant-multiple-representations}
\end{figure}


In the case of tumour genomes, the often high rate of mutations combined with possible high clonality and polyploidy generate further noise and complexity impeding the accurate determination of variants. In cancer studies one often wishes to distinguish the \emph{somatic} mutations -those acquired by the cells during tumorigenesis- from the individual's \emph{germline} variants that are present in every cell. To this end, normal tissue is often sequenced alongside the tumour material, and the resulting variant sets are compared to elucidate the set of mutations only present in the cancerous DNA. Therefore, having accurate variant callers and methods of comparison is of vital importance, but even methods designed specifically for the identification of somatic variants \cite{xu2014comparison,kim2013comparing,roberts2013comparative} show poor concordance \cite{alioto2015comprehensive, kroigaard2016evaluation, orawe2013}, and improvements are continually sought \cite{callari2017,vijayan2017}.

Structural variations are by definition large scale variants, and most sequencers use short reads, making SVs dificult to detect. Consequently, the concordance between different SV calling methods is even poorer than for the small variants \cite{sedlazeck2017}. This issue is compounded by a lack of a standard file format \cite{scherer2007} making comparisons between studies very difficult. The advent of third generation (long-read) sequencing offers prospects of improving SV detection \cite{sedlazeck2018,merker2017}. These methods typically have higher error rates than short read sequencing techniques, but by using a hybrid approach the advantages of long read sequencing can be combined with the accuracy of short reads to optimize detection of large scale genomics rearrangements \cite{fan2017hysa,weissensteiner2017, miller2017, ritz2014}.

\newpage
\thispagestyle{empty}
\begin{center}
\vspace{2cm}
\begin{minipage}{5in}
\tikz[remember picture,overlay]
\node[opacity=0.8,inner sep=0pt] at (current page.center){
    \includegraphics[width=\paperwidth,height=\paperheight]{chapters/images/background-texture-blue-reduced.jpg}
};
\sc
\begin{center}

\color{white}{
\Large The Human Microbiome \normalsize
\vspace{1cm}


\epigraph{5in}{By the means of Telescopes, there is nothing so far distant but may be represented to our view; and by the help of Microscopes, there is nothing so small as to escape our inquiry; hence there is a new visible World discovered to the understanding. By this means the Heavens are openâ€™d and a vast number of new Stars and new Motions, and new Productions appear in them, to which all the ancient Astronomers were utterly strangers. By this the Earth it self, which lyes so neer to us, under our feet, shews quite a new thing to us, and in every little particle of its matter, we now behold almost as great a variety of Creatures, as we were able before to reckon up in the whole Universe itself.}{Robert Hooke, 1665 (in the Preface of Micrographia)}



\vspace*{1cm}
\includegraphics[scale=2]{chapters/images/mycrobiota/animalcules2.png}

}

\end{center}
\end{minipage}
\end{center}

\newpage


\section{Use case 2: Microbiota profiling}

\begin{comment}

overview of reading materials about the human microbiome

http://www.richardsprague.com/note/2017/10/16/best-academic-papers-about-the-microbiome/

informal history of microbiology: https://www.bioexplorer.net/history_of_biology/microbiology/
\end{comment}


When Dutch inventor and scientist Antonie van Leeuwenhoek first turned his microscope to a drop of rain water and discovered within a wealth of microscopic life which he termed \textit{animalcules}, a whole new world of knowledge opened up.
It was subsequently discovered that while these organisms might be microscopic in size, their impact on human lives was enormous, causing food spoilage and disease. Through continued study over the following centuries, links between particular microorganisms and disease were discovered, and remedies developed. More recently, researchers have realised also the importance of not only studying the relationship between specific taxons and disease, but also of the structure and composition of microbiome \emph{as a whole} with respect to human health. This observation has led to the launch of large-scale research efforts such as the Human Microbiome Project \cite{turnbaugh2007human} and MetaHIT \cite{ehrlich2011metahit} to investigate the links between the microbiome and health.


\subsection{The Bio}

Every cell in our body contains a copy of our genome in its nucleus, which has been dubbed the blueprint of life. But these cells aren't the only source of genetic material in our bodies; each one of us harbour a vast amount of micro-organisms at various anatomical locations (Figure \ref{fig:microbiome});. Estimates put the number of of microbial cells as equal to or outnumbering human cells \cite{sender2016outnumbered}. These microorganisms influence our metabolism, impact our health and immunity, and can change drug efficacy. If our genomes are the blueprint of life, then our microbiomes are an important overlay to this blueprint; or, when we consider the genome as the \emph{source code} of life, the microbiome is an essential third-party plugin. For this reason, the microbiome is often referred to as our \emph{second genome} \cite{grice2012microbiome}, and has emerged in recent years as an important field of biomedical study. Links between the microbiome composition and diseases have been demonstrated in a number of diseases \cite{cho2012human} including psoriasis \cite{gao2008substantial}, obesity \cite{turnbaugh2006obesity, ley2005obesity}, and colorectal cancer \cite{castellarin2012fusobacterium,kostic2012genomic}, though in many cases a causal link remains to be proven.

\begin{figure}[h!]
    \centering
    \includegraphics[width=250pt]{chapters/images/mycrobiota/microbiome.jpg}
    \caption{Presence of microbiota at different anatomical site in the human body and in different compositions}
    \label{fig:microbiome}
\end{figure}

\subsection{The Informatics}

Sequencing the human microbiome comes with its own unique set of challenges. Because the microbiome typically consists of a large number of different organisms, this situation is akin to solving a hundreds or thousands of jigsaw puzzles simultaneously with all the pieces mixed together in a single box. Depending on the experimental setup, different approaches may be used to sequence the microbiome, such as amplicon or whole-genome sequencing.

\paragraph{16S rRNA amplicon sequencing} is used in situations where we are primarily  interested in the \emph{composition} of the microbiome, in discovering which species (or any other taxon) of organisms are present, and are less concerned about their exact genome sequences or any mutations that might be present therein. Consider for example patients with a bacterial infection which, depending on the exact type of bacteria present, may call for treatment with different antibiotics. Without sequencing, bacteria from such samples may be cultured in a Petri dish in order to identify the strains present; but with sequencing costs decreasing, 16S sequencing has become a viable alternative, with the added advantage that it is not restricted to aerobic organisms or suffer from any other constraints and biases imposed by the culturing process.

With 16S profiling, we do not sequence the entire genomes of the microorganisms, but only a single gene of each genome, the 16S rRNA gene. This gene is ideally suited for this purpose because it is present in all bacteria, but contains enough variability between different species to be able to distinguish between them. Because we sequence only a small stretch of DNA, this approach is much cheaper than were we to sequence all DNA. Think of this approach as looking only at the edge pieces of the jigsaw puzzles; You will not be able to see all the details of the puzzle, but you may have just enough information to deduce whether the picture on the box is a portrait or a landscape.


\paragraph{Whole-genome shotgun sequencing} is used in instances where we are interested not only in the composition of the microbiome, but also in the \emph{functional} characteristics. For example, antibiotic resistance of bacteria has become a major concern in recent years; bacteria evolve and obtain mutations that make antibiotics less effective, and identifying the presence of these mutations may influence diagnostics and treatment options. While this approach is more powerful, it is also more costly and the data analysis more complex.


\section{Scope of this Thesis}

Bioinformatics has become and integral part of any biomedical research, and vast amounts of -omics data are being generated. In contrast to other big-data producing fields such as astronomy, biologists are not typically trained in programming and command-line usage, which are essential skills for running bioinformatics analyses. In order to empower researchers in the biomedical field to analyse their own datasets, we

\textbf{Chapter 1} discusses the continued utility of the Galaxy platform to facilitate accessible, reproducible research.

In \textbf{Chapter 2} we discuss the infrastructure we created for a community-driven central repository of training materials using Galaxy as the teaching platform.

In \textbf{Chapter 3} we showcase tools created for the enhanced visualisation and reporting of the results of bioinformatics pipelines (iReport and Circos) (TODO: finish Circos paper, if not, add iReport to first chapter?)

In the remaining chapters, we illustrate this open and accessible approach to bioinformatics through a set of use cases. Each chapter contains a paper about a bioinformatic tool developed, and a paper about the biological study it enabled.

\textbf{Chapter 4} discusses the iFUSE tool developed for the detection and visualisation of fusion genes and Cancer, and how this was used to detect chromothripsis and several fusion genes in the VCaP prostate cancer cell line.

In \textbf{Chapter 5} we illustrate the Complete Genomics Galaxy Annotation Toolkit (CGtag) and its subsequent use in the developement of a virtual normal approach to somatic variant detection.

In \textbf{Chapter 6} we developed Galaxy mothur Toolkit (GmT), a suite of 125+ Galaxy tools, and used this to develop the MYcrbiota pipeline for microbiota profiling in sepsis patients for Streeklab Haarlem.



\end{justify}

\bibliographystyle{ieeetr}
\bibliography{references}
